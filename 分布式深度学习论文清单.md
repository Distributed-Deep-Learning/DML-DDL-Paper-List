# 分布式机器/深度学习论文清单







# 1.深度学习基础

#### 1.1 书籍

Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. "**Deep learning**." An MIT Press book. (2015). [[html\]](http://www.deeplearningbook.org/) **(Deep Learning Bible, you can read this book while reading following papers.)** （有中文翻译版本）



《深度学习入门》  斋藤康毅 [[pdf\]](https://vel.life/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E8%90%A5/%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8EPython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0%E3%80%8B%E9%AB%98%E6%B8%85%E4%B8%AD%E6%96%87%E7%89%88.pdf) 



《解析深度学习》  魏秀参 [[pdf\]](https://github.com/wx-chevalier/Awesome-CS-Books/blob/master/DataScienceAI/DeepLearning/2017-%E9%AD%8F%E7%A7%80%E5%8F%82-%E8%A7%A3%E6%9E%90%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5%E6%89%8B%E5%86%8C.pdf) 



《Machine Learning Yearning》 吴恩达[[pdf]](<https://deeplearning-ai.github.io/machine-learning-yearning-cn/>)[[github]](<https://github.com/deeplearning-ai/machine-learning-yearning-cn>)

#### 1.2 李宏毅 视频 + PPT

[[主页链接](<http://speech.ee.ntu.edu.tw/~tlkagk/courses.html>)]]

[[b站视频]](<https://www.bilibili.com/video/av9770302/?from=search&seid=18283918172737974846>)

#### 1.3 stanford DL Slides

[[主页链接]](<http://cs231n.stanford.edu/syllabus.html>)



#### 1.4 Survey

LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. "Deep learning." Nature 521.7553 (2015): 436-444. [[pdf\]](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf) 



#### 1.5 Imagenet model

Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." Advances in neural information processing systems. 2012. [[pdf\]](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (AlexNet, Deep Learning Breakthrough)



 Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [[pdf\]](https://arxiv.org/pdf/1409.1556.pdf) (VGGNet,Neural Networks become very deep!)



Szegedy, Christian, et al. "Going deeper with convolutions." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [[pdf\]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) (GoogLeNet)



He, Kaiming, et al. "Deep residual learning for image recognition." arXiv preprint arXiv:1512.03385 (2015). [[pdf\]](https://arxiv.org/pdf/1512.03385.pdf)(ResNet)



#### 1.6 模型调优，模型优化

Hinton, Geoffrey E., et al. "Improving neural networks by preventing co-adaptation of feature detectors." arXiv preprint arXiv:1207.0580 (2012). [[pdf\]](https://arxiv.org/pdf/1207.0580.pdf) (Dropout) 



Srivastava, Nitish, et al. "Dropout: a simple way to prevent neural networks from overfitting." Journal of Machine Learning Research 15.1 (2014): 1929-1958. [[pdf\]](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)



Ioffe, Sergey, and Christian Szegedy. "Batch normalization: Accelerating deep network training by reducing internal covariate shift." arXiv preprint arXiv:1502.03167 (2015). [[pdf\]](http://arxiv.org/pdf/1502.03167) 



Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. "Layer normalization." arXiv preprint arXiv:1607.06450 (2016). [[pdf\]](https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote) (Update of Batch Normalization)



Courbariaux, Matthieu, et al. "Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1." [[pdf\]](https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf) (New Model,Fast)



Jaderberg, Max, et al. "Decoupled neural interfaces using synthetic gradients." arXiv preprint arXiv:1608.05343 (2016). [[pdf\]](https://arxiv.org/pdf/1608.05343) **(Innovation of Training Method,Amazing Work)



Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. "Net2net: Accelerating learning via knowledge transfer." arXiv preprint arXiv:1511.05641 (2015). [[pdf\]](https://arxiv.org/abs/1511.05641) (Modify previously trained network to reduce training epochs)



Wei, Tao, et al. "Network Morphism." arXiv preprint arXiv:1603.01670 (2016). [[pdf\]](https://arxiv.org/abs/1603.01670) (Modify previously trained network to reduce training epochs)



Sutskever, Ilya, et al. "On the importance of initialization and momentum in deep learning." ICML (3) 28 (2013): 1139-1147. [[pdf\]](http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf) (Momentum optimizer)



Kingma, Diederik, and Jimmy Ba. "Adam: A method for stochastic optimization." arXiv preprint arXiv:1412.6980 (2014). [[pdf\]](http://arxiv.org/pdf/1412.6980) (Maybe used most often currently)



Andrychowicz, Marcin, et al. "Learning to learn by gradient descent by gradient descent." arXiv preprint arXiv:1606.04474 (2016). [[pdf\]](https://arxiv.org/pdf/1606.04474) (Neural Optimizer,Amazing Work) 



Han, Song, Huizi Mao, and William J. Dally. "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding." CoRR, abs/1510.00149 2 (2015). [[pdf\]](https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf) (ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup)



Iandola, Forrest N., et al. "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size." arXiv preprint arXiv:1602.07360 (2016). [[pdf\]](http://arxiv.org/pdf/1602.07360) (Also a new direction to optimize NN,DeePhi Tech Startup)





# 2. 框架

##### 2.1 tensorflow [[pdf\]](https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf) 

google 生态强大

##### 2.2 MXnet [[pdf\]](<https://www.cs.cmu.edu/~muli/file/mxnet-learning-sys.pdf>) 

李沐 陈天奇 亚马逊

##### 2.3 caffe [[pdf\]](<https://arxiv.org/pdf/1408.5093.pdf>) 

贾杨清

##### 2.4 pytorch [[官网\]](<https://pytorch.org/>) 

facebook 

##### 2.5 Angel [[pdf]](<https://pdfs.semanticscholar.org/f37f/387ddba906fba0ae81d1b323b08cd4e1fe59.pdf>)

腾讯与北大

##### 2.6 飞桨 [[github]](https://github.com/PaddlePaddle/Paddle)

百度



# 3.分布式



#### 3.1 gengral survey

Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis [[pdf]](https://arxiv.org/abs/1802.09941)

Scalable Deep Learning on Distributed Infrastructures: Challenges, Techniques and Tools[[pdf]](<https://arxiv.org/pdf/1903.11314.pdf>)



#### 3.2 参数服务器架构

Scaling Distributed Machine Learning with the Parameter Server. In Proc. 11th USENIX Conference on Operating Systems Design and Implementation (OSDI’14). 583–598. [[pdf](https://www.cs.cmu.edu/~dga/papers/osdi14-paper-li_mu.pdf)]



GeePS: Scalable Deep Learning on Distributed GPUs with a GPU-specialized Parameter. Server.[[pdf]](http://www.pdl.cmu.edu/PDL-FTP/CloudComputing/GeePS-cui-eurosys16.pdf)



Project Adam: Building an Efficient and Scalable Deep Learning Training System. In 11th USENIX Symposium on Operating Systems Design and Implementation. 571–582.[[pdf]](https://pdfs.semanticscholar.org/043a/fbd936c95d0e33c4a391365893bd4102f1a7.pdf)



Large Scale Distributed Deep Networks. In Proc. 25th International Conference on Neural Information Processing Systems - Volume 1 (NIPS’12). 1223–1231.[[pdf]](https://www.cs.toronto.edu/~ranzato/publications/DistBeliefNIPS2012_withAppendix.pdf)



Petuum: A New Platform for Distributed Machine Learning on Big Data. IEEE Transactions on Big Data 1, 2 (2015), 49–67.[[pdf]](http://www.cs.cmu.edu/~pengtaox/papers/petuum_15.pdf)



Parameter Hub- a Rack-Scale Parameter Server for Distributed Deep Neural Network Training    [[pdf]](<https://arxiv.org/pdf/1805.07891.pdf>)



#### 3.3 性能测评，性能建模

Performance Modeling and Evaluation of Distributed Deep Learning Frameworks on GPUs   [[pdf]](<https://arxiv.org/pdf/1711.05979.pdf>)



Performance Modeling and Scalability Optimization of Distributed Deep Learning Systems  [[pdf]](<https://arxiv.org/pdf/1612.00521.pdf>) [[slides]](<http://matsu-www.is.titech.ac.jp/lecture/lecture-wiki/index.php?plugin=attach&pcmd=open&file=20171110_hpc17.pdf&refer=hpc2017>)



Benchmarking State-of-the-Art Deep Learning Software Tools  [[pdf]](<https://www.comp.hkbu.edu.hk/~chxw/papers/ccbd_2016.pdf>)



Evaluation of Deep Learning Frameworks over Different HPC Architectures [[pdf]](<https://ieeexplore.ieee.org/document/7980078>)



An In-depth Performance Characterization of CPU- and GPU-based DNN Training on Modern Architectures  [[slides]](<http://mvapich.cse.ohio-state.edu/static/media/talks/slide/awan-mlhpc17.pdf>)



Modeling and Evaluation of Synchronous Stochastic Gradient Descent in Distributed Deep Learning on Multiple GPUs [[pdf]](<https://dl.acm.org/citation.cfm?id=3146356>)



PALEO: A PERFORMANCE MODEL FOR DEEP NEURAL NETWORKS[[pdf]](<https://openreview.net/pdf?id=SyVVJ85lg>)



#### 3.4 资源调度

Gandiva Introspective Cluster Scheduling for Deep Learning 



Towards Distributed Machine Learning in Shared Clusters A Dynamically-Partitioned Approach



Data Driven Resource Allocation for Distributed Learning



Dolphin: Runtime Optimization for Distributed Machine Learning



Large-scale cluster management at Google with Borg



SLAQ Quality-Driven Scheduling for Distributed Machine Learning



Online Job Scheduling in Distributed Machine Learning Clusters



Optimus-An Efficient Dynamic Resource Scheduler for Deep learning Clusters



Tiresias-A GPU Cluster Manager for Distributed Deep Learning



#### 3.5 数据放置

Exploiting iterative-ness for parallel ML computations



Tornado: A System For Real-Time Iterative Analysis Over Evolving Data



Runtime Data Layout Scheduling for Machine Learning Dataset



Graph Partitioning via Parallel Submodular Approximation to Accelerate Distributed Machine Learning



#### 3.6 Topo Map



Device Placement Optimization with Reinforcement Learning



Spotlight- Optimizing Device Placement for Training Deep Neural Networks



Communication-Aware Mapping of Stream Graphs for Multi-GPU Platforms



Topology-Aware GPU Scheduling for LearningWorkloads in Cloud Environments



#### 3.7 混合并行方案

SplitNet- Learning to Semantically Split Deep Networks



Unifying Data, Model and Hybrid Parallelism in Deep Learning via Tensor Tiling



Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks



Beyond Data and Model Parallelism for Deep Neural Networks





#### 3.8 内存管理

Layer-Centric Memory Reuse and Data Migration for Extreme-Scale Deep Learning on Many-Core Architectures



Profile-guided memory optimization for deep neural networks



SuperNeurons- Dynamic GPU Memory Management for Training Deep Neural Networks



vDNN- Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design





#### 3.9 梯度压缩

Seide F, Fu H, Droppo J, et al. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns[C]//Fifteenth Annual Conference of the International Speech Communication Association. INTERSPEECH 2014. 



Alistarh D, Grubic D, Li J, et al. QSGD: Communication-efficient SGD via gradient quantization and encoding[C]//Advances in Neural Information Processing Systems. NIPS 2017: 1709-1720.



Wen W, Xu C, Yan F, et al. Terngrad: Ternary gradients to reduce communication in distributed deep learning[C]//Advances in neural information processing systems. NIPS 2017: 1509-1519. 



Strom N. Scalable distributed DNN training using commodity GPU cloud computing[C]//Sixteenth Annual Conference of the International Speech Communication Association. INTERSPEECH 2015. 



Dryden N, Moon T, Jacobs S A, et al. Communication quantization for data-parallel training of deep neural networks[C]//Machine Learning in HPC Environments .MLHPC 2016, Workshop on. IEEE, 2016: 1-8.



Aji A F, Heafield K. Sparse communication for distributed gradient descent[J]. arXiv preprint arXiv:1704.05021, 2017.



Chen C Y, Choi J, Brand D, et al. AdaComp: Adaptive Residual Gradient Compression for Data-Parallel Distributed Training[J]. arXiv preprint arXiv:1712.02679, 2017.



#### 3.10 网络优化通信

Optimal Message Scheduling for Aggregation 



sysml18-Network Evolution for DNNs





#### 3.11 流水线

PipeDream Fast and Efficient Pipeline Parallel DNN Training



pipe-sgd a decentralized pipelined sgd framework for distributed deep net training



AMPNET ASYNCHRONOUS MODEL-PARALLELTRAINING FOR DYNAMIC NEURAL NETWORKS



Efficient and Robust Parallel DNN Training through Model Parallelism on Multi-GPU Platform



Pipelined Back-Propagation for Context-Dependent Deep Neural Networks



# 4.联邦学习



#### 4.1 系统优化：

TOWARDS FEDERATED LEARNING AT SCALE: SYSTEM DESIGN（SysML）

#### 4.2 设备选择：

Client Selection for Federated Learning with  Heterogeneous Resources in Mobile Edge

#### 4.3 通信：

DEEP GRADIENT COMPRESSION: REDUCING THE COMMUNICATION BANDWIDTH FOR  DISTRIBUTED TRAINING(ICLR)



Sparse Binary Compression: Towards Distributed Deep Learning with minimal Communication



Robust and Communication-Efficient Federated Learning from Non-IID Data



Federated Averaging__Communication-Efficient Learning of Deep Networks from Decentralized Data(AISTATS)



ADAPTIVE COMMUNICATION STRATEGIES TO ACHIEVE THE BEST  ERROR-RUNTIME TRADE-OFF IN LOCAL-UPDATE SGD(sysml2019)



Adaptive Task Allocation for Mobile Edge Learning



Adaptive Federated Learning in Resource Constrained Edge Computing Systems（INFOCOM）



Expanding the Reach of Federated Learning by Reducing Client Resource Requirements





#### 4.4 优化收敛：

Federated Optimization for Heterogeneous Networks



Federated Multi-Task Learning（NIPS）



Distributed Primal-Dual Optimization for Non-uniformly Distributed Data(IJCAI)





#### 4.5 安全：

How to Backdoor Federated Learning



Beyond Inferring Class Representatives: User-Level Privacy Leakage From Federated Learning(INFOCOM)





#### 4.6 资源管理：

Efficient Training Management for Mobile Crowd-Machine  Learning: A Deep Reinforcement Learning Approach



Collaborative Execution of Deep Neural Networks  on Internet of Things Devices



AdaLearner: An Adaptive Distributed Mobile Learning System for Neural Networks(ICCAD)



Federated Learning over Wireless Networks: Optimization Model Design and Analysis(INFOCOM)





#### 4.7 公平：

Agnostic Federated Learning(GOOGLE)



# 5. 会议推荐



偏系统：

[[SysML]](SysML: The New Frontier of Machine Learning Systems)

[[SOCC]]

[[OSDI]]

[[EuroSys]]

[[NeurIPS]]

[[ICML]]

[ICDCS]

[[SOSP]]



偏理论：

[[INFOCOM]]



偏底层：

[[SC]]

[[PPoPP]]

[[MICRO]]



# 6.资源，博客推荐



























